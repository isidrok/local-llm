# Ollama Configuration
# Choose one of the following setups:

# Option 1 - Standard Ollama (Docker or local)
BASE_URL=http://localhost:11434/v1
MODEL=qwen2.5:7b

# Option 2 - Docker llama.cpp (uncomment to use)
# BASE_URL=http://localhost:12434/engines/llama.cpp/v1
# MODEL=ai/qwen2.5:7B-Q4_0

# API Key (not needed for local Ollama)
API_KEY=not-needed