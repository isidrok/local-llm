# Ollama Configuration
# Choose one of the following setups:

# Option 1 - Standard Ollama (Docker or local)
#BASE_URL=http://localhost:11434/v1
#MODEL=gpt-oss:20b

# Option 2 - Docker llama.cpp (uncomment to use)
# BASE_URL=http://localhost:12434/engines/llama.cpp/v1
# MODEL=ai/gpt-oss

# Option 3 - Llama cpp server (uncomment to use)
BASE_URL=http://localhost:8080/v1
# MODEL=unsloth/gpt-oss-20b-GGUF:Q4_K_XL
MODEL=Qwen/Qwen3-14B-GGUF:Q4_K_M
# API Key (not needed for local Ollama)
API_KEY=not-needed